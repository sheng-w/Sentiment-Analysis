```{r}
library(tensorflow)
library(keras)
```

```{r}
#data_raw <- read.csv("Reviews.csv") #read the data
```
get the train and test, and validation sets
```{r}
data_raw_subset <- data_raw[1:20000,]
data <- data_raw_subset[, c("Summary", "Text", "Score")]#retain the summary, comment and score
data_size = dim(data)[1]
train_size = floor(data_size * 0.6)
test_size = floor(data_size * 0.2)
train = data[1:train_size,]
test = data[(train_size + 1):(train_size + test_size),]
validation = data[(train_size + test_size):data_size,]
```

运行以下代码块说明所有text文本中共有133039个不同的词。大部分词出现词数较少，暗示我们训练时可以用最高频的20000到40000个词。
```{r}
#u = text_tokenizer()
#fit_text_tokenizer(u, data_raw$Text)
#temp = texts_to_sequences(u, data_raw$Text)
#temp = unlist(temp)
#print(max(temp))  #输出133039
```
运行以下代码块中的代码，发现词数最多的评论有3507个词，从histogram发现大部分评论具有的词数较少，而百分之九十七点五的评论的词数在291以下。我们在训练时可以截取评论的前三百个词。
```{r}
#u = text_tokenizer()
#fit_text_tokenizer(u, data_raw$Text)
#temp = texts_to_sequences(u, data_raw$Text)
#temp2 = lapply(temp, FUN = length)
#temp2 = unlist(temp2)
#hist(temp2,breaks = 100)
#print(max(temp2))
#quantile(temp2, prob = 0.675)
```

preprocess the data.
```{r}
u = text_tokenizer(num_words = 30000)
fit_text_tokenizer(u, data$Text)


x_train = texts_to_sequences(u, train$Text)
y_train = train$Score

x_test = texts_to_sequences(u, test$Text)
y_test = test$Score

x_validation = texts_to_sequences(u, validation$Text)
y_validation = validation$Score

to_star_matrix = function(y) 
{
  if(is.matrix(y)) {
    return(y)
  }
  l = length(y)
  s = matrix(0, nrow = l, ncol = 5)
  for(i in 1:l) {
    ind = y[i]
    #if(ind != 0)
    #{
    for(j in 1:ind)
    {
     # s[i, y[i]] = 1
     s[i, j] = 1
    }
    #}  
  }
  return(s)
}

x_train = pad_sequences(x_train, 300)
x_test = pad_sequences(x_test, 300)
x_validation = pad_sequences(x_validation, 300)

y_train = to_star_matrix(y_train)
y_test = to_star_matrix(y_test)
y_validation = to_star_matrix(y_validation)
```


fit the neural network
```{r}
model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim = 30000, output_dim = 200) %>%
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = TRUE) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
  layer_dense(units = 5, activation = 'sigmoid')

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = 'adam',
  metrics = c('accuracy')
)

model %>% keras::fit(
  x_train, y_train,
  epochs = 1,
  batch_size = 50,
  validation_data = list(x_test, y_test)
)


predict(model, x_train[1:20,])
head(y_train, n = 20)
temp = predict(model, x_validation)

```




```{r}
data_raw_subset <- data_raw[1:20000,]
data <- data_raw_subset[, c("Summary", "Text", "Score")]#retain the summary, comment and score
data_size = dim(data)[1]
train_size = floor(data_size * 0.6)
test_size = floor(data_size * 0.2)
train = data[1:train_size,]
test = data[(train_size + 1):(train_size + test_size),]
validation = data[(train_size + test_size):data_size,]

u = text_tokenizer(num_words = 30000)
fit_text_tokenizer(u, data$Summary)


x_train_s = texts_to_sequences(u, train$Summary)
y_train_s = train$Score

x_test_s = texts_to_sequences(u, test$Summary)
y_test_s = test$Score

x_validation_s = texts_to_sequences(u, validation$Summary)
y_validation_s = validation$Score

to_star_matrix = function(y) 
{
  if(is.matrix(y)) {
    return(y)
  }
  l = length(y)
  s = matrix(0, nrow = l, ncol = 5)
  for(i in 1:l) {
    ind = y[i]
    for(j in 1:ind)
    {
     # s[i, y[i]] = 1
     s[i, j] = 1
    }
  }
  return(s)
}

x_train_s = pad_sequences(x_train_s, 50)
x_test_s = pad_sequences(x_test_s, 50)
x_validation_s = pad_sequences(x_validation_s, 50)

y_train_s = to_star_matrix(y_train_s)
y_test_s = to_star_matrix(y_test_s)
y_validation_s = to_star_matrix(y_validation_s)
```


```{r}
model_s <- keras_model_sequential()
model_s %>%
  layer_embedding(input_dim = 30000, output_dim = 100) %>%
  layer_lstm(units = 32, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = TRUE) %>% 
  layer_lstm(units = 32, dropout = 0.2, recurrent_dropout = 0.2) %>%
  layer_dense(units = 5, activation = 'sigmoid')

model_s %>% compile(
  loss = "categorical_crossentropy",
  optimizer = 'adam',
  metrics = c('accuracy')
)

model_s %>% keras::fit(
  x_train_s, y_train_s,
  epochs = 1,
  batch_size = 50,
  validation_data = list(x_test_s, y_test_s)
)


predict(model_s, x_train_s[1:20,])
head(y_train_s, n = 20)
temp_s = predict(model_s, x_validation_s)

```

```{r}
require(xgboost)
feature_neural_text = predict(model, x_train)
feature_neural_summary = predict(model_s, x_train_s)
data_tree = data_raw_subset[c("HelpfulnessNumerator", "HelpfulnessDenominator")]
data_tree_rate = 0
data_tree_size = dim(data_tree)[1]
for(i in 1:data_tree_size)
{
  if(data_tree$HelpfulnessNumerator[i] != 0)
  {
    data_tree$Rate[i] = data_tree$HelpfulnessNumerator[i]/data_tree$HelpfulnessDenominator[i]
  }
}
feature_neural_text = as.data.frame(feature_neural_text)
feature_neural_summary = as.data.frame(feature_neural_summary)
#data_tree_all = cbind(data_tree[1:12000, ], feature_neural_summary, feature_neural_text)
data_tree_all = cbind(data_tree[1:12000, ], feature_neural_summary)
data_tree_all = cbind(data_tree[1:12000, ],feature_neural_text)
```

```{r}
y_train_tree = train$Score
bst <- xgboost(data = as.matrix(data_tree_all), label = y_train_tree - 1, max.depth = 5,  
               eta = 0.5, nthread = 2, nround = 10,objective = "multi:softmax", num_class = 5) 

```

```{r}
p = predict(bst, as.matrix(data_tree_all)) + 1
head(p, n = 100)
head(y_train_tree, n = 100)
print(sum(p == y_train_tree)/length(y_train_tree))
```

```{r}
feature_neural_text_test = predict(model, x_test)
feature_neural_summary_test = predict(model_s, x_test_s)
data_tree = data_raw_subset[c("HelpfulnessNumerator", "HelpfulnessDenominator")][12001:16000, ]
data_tree_rate = 0
data_tree_size = dim(data_tree)[1]
for(i in 1:data_tree_size)
{
  if(data_tree$HelpfulnessNumerator[i] != 0)
  {
    data_tree$Rate[i] = data_tree$HelpfulnessNumerator[i]/data_tree$HelpfulnessDenominator[i]
  }
}
feature_neural_text_test = as.data.frame(feature_neural_text_test)
feature_neural_summary_test = as.data.frame(feature_neural_summary_test)
#data_tree_all = cbind(data_tree[1:12000, ], feature_neural_summary, feature_neural_text)
data_tree_all = cbind(data_tree, feature_neural_summary_test)
#data_tree_all = cbind(data_tree,feature_neural_summary_test, feature_neural_text_test)

```

```{r}
y_test_tree = test$Score
bst <- xgboost(data = as.matrix(data_tree_all), label = y_test_tree - 1, max.depth = 5,  
               eta = 0.5, nthread = 2, nround = 10,objective = "multi:softmax", num_class = 5) 

```

```{r}
p = predict(bst, as.matrix(data_tree_all)) + 1
head(p, n = 100)
head(y_test_tree, n = 100)
print(sum(p == y_test_tree)/length(y_test_tree))

```


